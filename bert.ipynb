{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38ff766-d3dc-4971-8e29-34e0ff49b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "import emoji\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30830c36-3097-4aed-889b-a0db236a0432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35326558-cb85-4fda-be6f-543660882ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42359c81-fb33-4f7a-abcb-710705dc4716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c1a285-9f55-43ae-a17f-c697abc6a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = emoji.demojize(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52dee97-877d-4025-b352-7b2989f9e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(preprocess_text)\n",
    "test['text'] = test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfcba2c-0f4f-4d27-a38e-6f1c0b734e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07da0b6c-cec6-4596-8872-e39ac4620652",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f3f00ef-7b0e-49b6-a535-c73917b0f82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, hidden_dropout_prob=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31a3731f-51b1-4cd9-a9ca-d9287414a127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0dd4e6a-83a8-4c9a-84a2-3d364057a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['text'].values.tolist() \n",
    "y = train['target'].values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0bbedcf-2d30-483d-bb5f-d9cc3c80cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X,y, train_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8549d7e-8eb1-409c-8a89-c79881a86887",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2e6bfc1-bdd9-4378-8a19-45f439e6e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "684af2e6-8b22-4548-a2f0-674f103d0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04676605-2804-41e8-8aaf-a0af9030ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a31a0605-901a-4c33-bdad-f1311d0f88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208ec5b3-e7df-474d-bb3b-73df20c64866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs):\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_predictions = []\n",
    "        train_true_labels = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc='Training'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_predictions.extend(predictions.cpu().numpy())\n",
    "            train_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
    "        \n",
    "        print(f'Average training loss: {avg_train_loss:}')\n",
    "        print(f'Training accuracy: {train_accuracy:}')\n",
    "        print(classification_report(train_true_labels, train_predictions))\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                loss = criterion(logits, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                val_predictions.extend(predictions.cpu().numpy())\n",
    "                val_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f'\\nAverage validation loss: {avg_val_loss}')\n",
    "        print(f'Validation accuracy: {val_accuracy}')\n",
    "        print(classification_report(val_true_labels, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f7036f5-9457-4391-9f87-aaf573852bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 36/36 [00:25<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.640159995191627\n",
      "Training accuracy: 0.6202365308804205\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.60      0.64      1277\n",
      "           1       0.56      0.65      0.60      1006\n",
      "\n",
      "    accuracy                           0.62      2283\n",
      "   macro avg       0.62      0.62      0.62      2283\n",
      "weighted avg       0.63      0.62      0.62      2283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 84/84 [00:17<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation loss: 0.5251189182911601\n",
      "Validation accuracy: 0.7637898686679174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81      3065\n",
      "           1       0.76      0.65      0.70      2265\n",
      "\n",
      "    accuracy                           0.76      5330\n",
      "   macro avg       0.76      0.75      0.75      5330\n",
      "weighted avg       0.76      0.76      0.76      5330\n",
      "\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 36/36 [00:23<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5011729117896822\n",
      "Training accuracy: 0.7744196233026719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      1277\n",
      "           1       0.75      0.73      0.74      1006\n",
      "\n",
      "    accuracy                           0.77      2283\n",
      "   macro avg       0.77      0.77      0.77      2283\n",
      "weighted avg       0.77      0.77      0.77      2283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 84/84 [00:17<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation loss: 0.46810848727112725\n",
      "Validation accuracy: 0.799812382739212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83      3065\n",
      "           1       0.76      0.77      0.77      2265\n",
      "\n",
      "    accuracy                           0.80      5330\n",
      "   macro avg       0.80      0.80      0.80      5330\n",
      "weighted avg       0.80      0.80      0.80      5330\n",
      "\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 36/36 [00:23<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.43758362614446217\n",
      "Training accuracy: 0.8177836180464302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84      1277\n",
      "           1       0.82      0.75      0.78      1006\n",
      "\n",
      "    accuracy                           0.82      2283\n",
      "   macro avg       0.82      0.81      0.81      2283\n",
      "weighted avg       0.82      0.82      0.82      2283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 84/84 [00:17<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation loss: 0.4539369028948602\n",
      "Validation accuracy: 0.8120075046904315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84      3065\n",
      "           1       0.80      0.75      0.77      2265\n",
      "\n",
      "    accuracy                           0.81      5330\n",
      "   macro avg       0.81      0.80      0.81      5330\n",
      "weighted avg       0.81      0.81      0.81      5330\n",
      "\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████| 36/36 [00:23<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.40355272094408673\n",
      "Training accuracy: 0.8287341217696014\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85      1277\n",
      "           1       0.83      0.77      0.80      1006\n",
      "\n",
      "    accuracy                           0.83      2283\n",
      "   macro avg       0.83      0.82      0.82      2283\n",
      "weighted avg       0.83      0.83      0.83      2283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 84/84 [00:17<00:00,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average validation loss: 0.46409895750028746\n",
      "Validation accuracy: 0.8078799249530957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83      3065\n",
      "           1       0.77      0.78      0.78      2265\n",
      "\n",
      "    accuracy                           0.81      5330\n",
      "   macro avg       0.80      0.80      0.80      5330\n",
      "weighted avg       0.81      0.81      0.81      5330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc24967a-c39c-4043-ad00-50d297ffbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TDataset(test['text'].values.tolist(), [-1] * len(test), tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad1200ae-2f20-45d9-9722-9517c3840a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    model.eval()  \n",
    "    predictions = []\n",
    "    with torch.no_grad():  \n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(batch_predictions.cpu().numpy()) \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adeef0ec-56bd-4138-9a45-9b31a7b3cf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|█████████████████████████████████████████████████████████████████████████| 51/51 [00:08<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e71c61f4-c211-42bb-97f2-e09e360bf7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test['id'], 'target': test_predictions})\n",
    "submission.to_csv('submission_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6520da-8dc9-49bf-b93d-22fd4ff91a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
